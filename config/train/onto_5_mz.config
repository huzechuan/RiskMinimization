[Task]
transfer=True
task=Onto
metric=f1

[Model]
use_bert=1
model_type=SoftmaxX2
softem=True
num_decoders=5
dropout=0.5
word_embeddings=conll_
use_char=False
load_model=True
load_model_path=.transfer_data/Onto/bc/softmax_mbert/bc/best_model.pt,.transfer_data/Onto/bn/softmax_mbert/bn/best_model.pt,.transfer_data/Onto/nw/softmax_mbert/nw/best_model.pt,.transfer_data/Onto/tc/softmax_mbert/tc/best_model.pt,.transfer_data/Onto/wb/softmax_mbert/wb/best_model.pt
# ./results/ner/3/softmax_mbert/conll_03_english/best_model.pt

[Encoder]
num_layers=1
hidden_size=256

[Decoder]
num_layers=1
hidden_size=256

[Data]
data_root=./.transfer_data
embedding_root=./.data/embeddings
corpus=bc_bn_nw_tc_wb_to_mz
domain=OntoDomain
dataformat=conllu
result_root=./
sample=1.0
table=multi
# table=gold_row_encoder1


[Training]
rounds=1
batch=16
epochs=5
patience=0
mu=2
anneal_method=max
L2=1e-8
lr=5e-5
top_lr=2e-3
lr_decay=0.5
# mode= train eval tune
mode=train

[Visualization]
tensor_path=ner_test
tensorboard=False

[Tune_Int]
HP_tag_dim=20
HP_rank=384,256,128,64

[Tune_Float]
