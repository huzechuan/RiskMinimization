[Task]
transfer=True
task=ner
metric=f1

[Model]
use_bert=1
model_type=SoftmaxX2
softem=True
num_decoders=3
dropout=0.5
word_embeddings=conll_
use_char=False
load_model=True
load_model_path=.transfer_data/ner/conll_03_english/softmax_mbert/en/best_model1.pt,.transfer_data/ner/conll_03_dutch/softmax_mbert/nl/best_model1.pt,.transfer_data/ner/conll_03_spanish/softmax_mbert/es/best_model1.pt
# ./results/ner/3/softmax_mbert/conll_03_english/best_model.pt

[Encoder]
num_layers=1
hidden_size=256

[Decoder]
num_layers=1
hidden_size=256

[Data]
data_root=./.transfer_data
embedding_root=./.data/embeddings
corpus=en_nl_es_to_conll_03_german
domain=CoNLL
dataformat=conllu
result_root=./
sample=1.0
table=multi
# table=gold_row_encoder1


[Training]
rounds=1
batch=16
epochs=5
patience=0
mu=2
anneal_method=max
L2=1e-8
lr=5e-5
top_lr=2e-3
lr_decay=0.5
# mode= train eval tune
mode=train

[Visualization]
tensor_path=ner_test
tensorboard=False

[Tune_Int]
HP_tag_dim=20
HP_rank=384,256,128,64

[Tune_Float]
